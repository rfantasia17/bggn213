---
title: "Class08_miniproject"
author: "R(PID:A59010419"
date: "10/27/2021"
output: pdf_document
---

```{r}
# Complete the following code to input the data and store as wisc.df
fna.data <- "WisconsinCancer.csv"
wisc.df <- read.csv(fna.data, row.names=1)
#print(wisc.df)

# We can use -1 here to remove the first column which is the answer 
wisc.data <- wisc.df[,-1]
diagnosis<-as.factor(wisc.df$diagnosis)

#check out the data 
dim(wisc.data)
nrow(wisc.data)
# Q1. How many observations are in this dataset?
nrow(wisc.data)

#Q2. How many of the observations have a malignant diagnosis?
table(diagnosis)
#Q3. How many variables/features in the data are suffixed with _mean?
length(grep('_mean$',colnames(wisc.data)))
```
```{r}
colMeans<-colMeans(wisc.data)
colsd<-apply(wisc.data, c(2),sd)

#PCA on wisc.data 
wisc.pr <- prcomp(wisc.data,scale=TRUE)


summary(wisc.pr)
#Q4. From your results, what proportion of the original variance is captured by the first principal components (PC1)?
#0.4427
#Q5. How many principal components (PCs) are required to describe at least 70% of the original variance in the data?
# 3PCS 
#Q6. How many principal components (PCs) are required to describe at least 90% of the original variance in the data?
#7PCS
```

```{r}
biplot(wisc.pr)
#Q7. What stands out to you about this plot? Is it easy or difficult to understand? Why?
#It is impossible to interpret because there are too many labels and its obnoxious.  

plot(wisc.pr$x[,1:2], col = diagnosis , 
     xlab = "PC1", ylab = "PC2")
#Q8. Generate a similar plot for principal components 1 and  What do you notice about these plots?
plot(wisc.pr$x[,1],wisc.pr$x[,3], col = diagnosis , 
     xlab = "PC1", ylab = "PC3")


```


```{r}
# Create a data.frame for ggplot
df <- as.data.frame(wisc.pr$x)
df$diagnosis <- diagnosis

# Load the ggplot2 package
library(ggplot2)

# Make a scatter plot colored by diagnosis
library(ggplot2)
ggplot(df) + 
  aes(x=PC1, y=PC2, col=df$diagnosis) + 
  geom_point()
```
```{r}
pr.var <- wisc.pr$sdev^2
head(pr.var)

# Variance explained by each principal component: pve
pve <- pr.var /sum(pr.var)

# Plot variance explained for each principal component
plot(pve, xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     ylim = c(0, 1), type = "o")
```
```{r}
barplot(pve, ylab = "Precent of Variance Explained",
     names.arg=paste0("PC",1:length(pve)), las=2, axes = FALSE)
axis(2, at=pve, labels=round(pve,2)*100 )
```
```{r}
#Q9. For the first principal component, what is the component of the loading vector (i.e. wisc.pr$rotation[,1]) for the feature concave.points_mean?
print(wisc.pr$rotation[,1])


loading=as.factor(wisc.pr$rotation[,1])

loading['concave.points_mean']

#Q10. What is the minimum number of principal components required to explain 80% of the variance of the data
var<-summary(wisc.pr)
sum(var$importance[3,]<0.8)

```
```{r}
data.scaled <- scale(wisc.data)
data.dist <- dist(data.scaled)
wisc.hclust <- hclust(data.dist, method='complete')
plot(wisc.hclust)
abline(h=19, col="red", lty=2)

wisc.hclust.clusters <- cutree(wisc.hclust,4)
table(wisc.hclust.clusters, diagnosis)
```


```{r}
wisc.hclust.clusters <- cutree(wisc.hclust,2)
print(table(wisc.hclust.clusters, diagnosis))
wisc.hclust.clusters <- cutree(wisc.hclust,3)
print(table(wisc.hclust.clusters, diagnosis))
wisc.hclust.clusters <- cutree(wisc.hclust,4)
print(table(wisc.hclust.clusters, diagnosis))
wisc.hclust.clusters <- cutree(wisc.hclust,5)
print(table(wisc.hclust.clusters, diagnosis))
wisc.hclust.clusters <- cutree(wisc.hclust,6)
print(table(wisc.hclust.clusters, diagnosis))
wisc.hclust.clusters <- cutree(wisc.hclust,7)
print(table(wisc.hclust.clusters, diagnosis))
wisc.hclust.clusters <- cutree(wisc.hclust,8)
print(table(wisc.hclust.clusters, diagnosis))
wisc.hclust.clusters <- cutree(wisc.hclust,9)
print(table(wisc.hclust.clusters, diagnosis))
wisc.hclust.clusters <- cutree(wisc.hclust,10)
print(table(wisc.hclust.clusters, diagnosis))
```
Q12. Can you find a better cluster vs diagnoses match by cutting into a different number of clusters between 2 and 10?
4 and 5 look better.  They have good separation between B and M and they are not a large number of clusters that is artifiaclly clustering the data.


Q13. Which method gives your favorite results for the same data.dist dataset? Explain your reasoning.

I prefer the PCA analysis because hierarchical clustering seems less quantitative while PCA tells me how many PCAs represent a percentage of variance in my data and is quantitative. 
